{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObJmcyuYti5X"
      },
      "source": [
        "# Coursework 2: Data Processing\n",
        "\n",
        "## Task 1\n",
        "This coursework will assess your understanding of using NoSQL to store and retrieve data.  You will perform operations on data from a collection of scientific articles in a MongoDB database.  You will be required to run code to answer the given questions in the Jupyter notebook provided.\n",
        "\n",
        "\n",
        "Download the dataset articles.json from Blackboard and import it into a collection called 'articles' into the 'coursework' database. The Virtual Machine comes pre-configured with an user \"coursework\" with password \"coursework\" with read-write privileges over the coursework database. The admin database is also 'coursework'. \n",
        "\n",
        "Each question asks you to implement a function following a certain specification. We have an evaluation script that will check the correctness of your answer. It is essential that:\n",
        " \n",
        " 1. You respect the requested answer format, otherwise, the script will flag your answer as incorrect. If the result is correct, but your formatting is incorrect, you get a penalty of -20% of the mark.\n",
        " \n",
        " 2. Do not insert or delete cells in the notebook you will submit, the evaluation script relies on your answer being in the right cell.\n",
        "    \n",
        "    1. If you feel the need of inserting cells to prepare and test your answer, do it in a copy of the notebook. Once you finish an answer that fits in a single cell, transfer it to the notebook that you will submit.\n",
        "    \n",
        "    2. You can define auxiliary functions if you want, as long as they are all in the same cell as the main function.\n",
        "    \n",
        "    3. If you add print statements for testing, please delete or comment them before submitting.\n",
        "\n",
        "\n",
        "Answers can make use of the tools you learned so far: MongoDB pipelines and operators, Python code, pandas and networkx. An important aspect of the coursework is to identify in what situations is best to use MongoDB only, and in what situations is best to use multiple tools. A bad choice may make your answer less efficient, and in some cases, not run at all. The marking scheme at the end of the notebook details the expected execution times of your answers. Less efficient answers will get less marks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nr8-oztti5b"
      },
      "source": [
        " ## Context\n",
        "    \n",
        "Bibliometrics Ltd. has hired you, dear data scientist, to analyse the latest articles in Computer Science collected by the DBLP (https://dblp.uni-trier.de/) open bibliographic service and further processed by the AMiner team (https://www.aminer.cn/citation). You have been provided with the articles from 2017 and 2019 and an MS Teams chat window with \"The Management\", on which you will get the questions you need to answer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T05:09:30.669585Z",
          "start_time": "2021-12-16T05:09:30.659324Z"
        },
        "tags": [
          "import"
        ],
        "id": "vB1VxOpXti5c"
      },
      "outputs": [],
      "source": [
        "#import section\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "import networkx as nx\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q0"
        ],
        "id": "c--k4tzJti5d"
      },
      "source": [
        "### 0) \n",
        "\n",
        "Before the first question pops in the chat window, you better off set up everything to provide answers.\n",
        "\n",
        "Task:\n",
        "\n",
        "Write a function that given the name of a database and the name of a collection returns a pymongo collection object of the collection using the user you just created. Use this function to create a pymongo collection object of the 'articles' collection.\n",
        "\n",
        "\n",
        "[5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T05:09:33.145737Z",
          "start_time": "2021-12-16T05:09:33.124929Z"
        },
        "nbgrader": {
          "grade": false,
          "grade_id": "get_collection",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "tags": [
          "a0"
        ],
        "id": "5dR3Emp4ti5d"
      },
      "outputs": [],
      "source": [
        "def get_collection(dbname,collection_name):\n",
        "    \"\"\"\n",
        "    Connects to the server, and returns a collection object\n",
        "    of the `collection_name` collection in the `dbname` database\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "    client = MongoClient('mongodb://'+dbname+\":\"+dbname+'@'+'localhost:27017',authSource=dbname)\n",
        "    db = client[dbname]\n",
        "    return db[collection_name]\n",
        "\n",
        "# the collection on which you will work on\n",
        "articles = get_collection('coursework','articles')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q1"
        ],
        "id": "lm8FUXCRti5e"
      },
      "source": [
        "### 1) \n",
        "\n",
        "The Management has logged in... they want to know how many articles of each type according to the 'doc_type' property are in the dataset. However, being the competent data scientist you are, you know that you always need to verify your data before start crunching numbers...\n",
        "\n",
        "### Task:\n",
        "\n",
        "Write a function that returns the number of articles missing the 'doc_type' property or having it equal to the empty string (\"\") \n",
        "\n",
        "[5 points] \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T05:09:58.112073Z",
          "start_time": "2021-12-16T05:09:58.102312Z"
        },
        "nbgrader": {
          "grade": false,
          "grade_id": "get_bcced_people",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "tags": [
          "a1"
        ],
        "id": "cW037YBFti5e"
      },
      "outputs": [],
      "source": [
        "def count_missing_doc_types(articles):\n",
        "    \"\"\"\n",
        "    :param articles A PyMongo collection of articles\n",
        "    :return: int: Number or articles without a a 'doc_type' property or having it equal to the empty string ('') \n",
        "    \"\"\"    \n",
        "    # YOUR CODE HERE \n",
        "    find = {\"$or\": [{'doc_type': None}, {'doc_type': ''}]}\n",
        "    doc_count = articles.count_documents(find)\n",
        "    return doc_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q2"
        ],
        "id": "7ZDw-8Ydti5f"
      },
      "source": [
        "### 2) \n",
        "\n",
        "You inform The Management of how many articles are missing the doc_type property or have it empty. They would like to fix as many as possible before moving forward. They may be able to infer the missing types based on the article publisher, so they ask: what are the publishers of the articles that are missing the doc_type property?\n",
        "\n",
        "### Task:\n",
        "\n",
        "Write a function that returns the set of publishers of articles that are missing the doc_type property or have it equal to the empty string. \n",
        "\n",
        "[5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T05:09:45.114951Z",
          "start_time": "2021-12-16T05:09:45.098253Z"
        },
        "tags": [
          "a2"
        ],
        "id": "goXcshNkti5f"
      },
      "outputs": [],
      "source": [
        "def get_publishers_of_articles_missing_type(articles):\n",
        "    \"\"\"\n",
        "    :param articles PyMongo collection of articles\n",
        "    :return: Set: set of publishers of articles that are missing the doc_type property, defined as not having the field at all, or having it equal to the empty string.\n",
        "    \"\"\"    \n",
        "    # YOUR CODE HERE\n",
        "    setOfPublishers = set()\n",
        "    find = {\"$or\": [{'doc_type': None}, {'doc_type': ''}]}\n",
        "    cols = {\"_id\": 0, \"publisher\": 1}\n",
        "    documents = list(articles.find(find, cols))\n",
        "    for doc in documents:\n",
        "        if 'publisher' in doc:\n",
        "            setOfPublishers.add(doc['publisher'])\n",
        "    return setOfPublishers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q3"
        ],
        "id": "YyyPXIbIti5g"
      },
      "source": [
        "### 3)\n",
        "\n",
        "The Management analysed the set you provided in the previous task and decided that:\n",
        "\n",
        " 1) Articles missing doc_type  with publisher == 'Springer, Cham' should have 'doc_type' == 'Book'\n",
        " \n",
        " 2) Articles missing doc_type with publisher  == 'RFC Editor' should have 'doc_type' == 'RFC'\n",
        " \n",
        " 3) The remainder of articles missing doc_type should have 'doc_type' == 'N/A'\n",
        " \n",
        "Task:\n",
        "\n",
        "Write a function that updates the article collection according to The Management's decision.\n",
        "\n",
        "[5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T05:10:10.195370Z",
          "start_time": "2021-12-16T05:10:10.180141Z"
        },
        "scrolled": true,
        "tags": [
          "a3"
        ],
        "id": "hQ7ogcEvti5g"
      },
      "outputs": [],
      "source": [
        "def update_doc_types(articles):\n",
        "    \"\"\"\n",
        "    :param articles: PyMongo collection of articles\n",
        "    :return: PyMongo UpdateResult object \n",
        "    \"\"\"    \n",
        "    # YOUR CODE HERE\n",
        "    from pymongo import UpdateMany\n",
        "    updateRequests = [UpdateMany( {\"$and\": [{\"$or\": [{'doc_type': ''}, {'doc_type': None}]} , {'publisher': 'Springer, Cham'}] }, {\"$set\": {\"doc_type\": \"Book\"}}),\n",
        "                      UpdateMany( {\"$and\": [{\"$or\": [{'doc_type': ''}, {'doc_type': None}]} , {'publisher': 'RFC Editor'}]},{\"$set\": {\"doc_type\": \"RFC\"}}),\n",
        "                      UpdateMany( {\"$or\": [{'doc_type': ''}, {'doc_type': None}]} , {\"$set\": {\"doc_type\": \"N/A\"}})]\n",
        "    \n",
        "    updateResult = articles.bulk_write(updateRequests, ordered=True)\n",
        "\n",
        "    return updateResult.bulk_api_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q4"
        ],
        "id": "NyNurDi7ti5h"
      },
      "source": [
        "### 4)\n",
        "\n",
        "You are finally ready to answer the original question: What is the distribution of document types in the dataset?\n",
        "\n",
        "Task:\n",
        "\n",
        "Write a function that returns a dictionary with doc_types as keys and the number of articles of each type as values.\n",
        "\n",
        "e.g:\n",
        "\n",
        "{'Conference' : 4566 , 'N/A' : 7992 ...}\n",
        "\n",
        "\n",
        "\n",
        "[5 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:21:33.174270Z",
          "start_time": "2021-12-16T04:21:33.166320Z"
        },
        "tags": [
          "a4"
        ],
        "id": "BqSprJd-ti5h"
      },
      "outputs": [],
      "source": [
        "def get_types_distribution(articles):\n",
        "    \"\"\"\n",
        "    :param articles: PyMongo collection of articles\n",
        "    :return: Dictionary with article types as keys, and number of articles of type as value\n",
        "    \"\"\" \n",
        "    # YOUR CODE HERE\n",
        "    group = {'$group': {'_id': '$doc_type', 'noOfDocs': {'$sum': 1}}}\n",
        "    documents = list(articles.aggregate([group]))\n",
        "    keyList = []\n",
        "    valueLst = []\n",
        "    for doc in documents:\n",
        "        keyList.append(doc[\"_id\"])\n",
        "        valueLst.append(doc[\"noOfDocs\"])\n",
        "    docTypeCountDict = dict(zip(keyList, valueLst))\n",
        "    return docTypeCountDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q5"
        ],
        "id": "I0R4Z32Vti5h"
      },
      "source": [
        "###  5)\n",
        "\n",
        "Do longer articles (in number of pages) have more references?\n",
        "\n",
        "Task:\n",
        "\n",
        "Return an histogram dictionary with the following specification:\n",
        "\n",
        "{\n",
        "\"1-5\" : Average references of articles between 1 and 5 pages inclusive\n",
        "\n",
        "\"6-10\" : Average references of articles between 6 and 10 pages inclusive\n",
        "\n",
        "\"11-15\" : Average references of articles between 11 and 15 pages inclusive\n",
        "\n",
        "\"16-20\" : Average references of articles between 16 and 20 pages inclusive\n",
        "\n",
        "\"21-25\" : Average references of articles between 21 and 25 pages inclusive\n",
        "\n",
        "\"26-30\" : Average references of articles between 26 and 30 pages inclusive\n",
        "\n",
        "\">30\" : Average references of articles with more than 30 pages\n",
        "}\n",
        "\n",
        "A fellow data scientist of your team has found that some articles have no 'references' field, while others have unusually large page numbers. The Management decides that for this task you should\n",
        "\n",
        "* Ignore articles that are missing the 'references' field\n",
        "* Ignore articles with page numbers greater than 6 digits\n",
        "* Ignore articles with page numbers that cannot be parsed to integers (e.g. 900467-12)\n",
        "* Remember articles that start and end at the same page have 1 page, not zero!\n",
        "\n",
        "\n",
        "[10 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:22:16.235985Z",
          "start_time": "2021-12-16T04:22:16.222098Z"
        },
        "nbgrader": {
          "grade": false,
          "grade_id": "get_percentage_sent_on_weekend",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "scrolled": false,
        "tags": [
          "a5"
        ],
        "id": "zM0URYTpti5h"
      },
      "outputs": [],
      "source": [
        "\n",
        "def length_vs_references(articles):\n",
        "    \"\"\"\n",
        "    :param collection A PyMongo collection object\n",
        "    :return dictionary in the form described above\n",
        "    \"\"\"    \n",
        "    histDict= {}\n",
        "    checkReferences = {'$match': {'references': { '$exists' : True}, 'pageStatus' : True }}\n",
        "    projectPageDetails = {'$project' : { 'pageStatus' : { '$and' : [{'$regexMatch': {'input': \"$page_start\",'regex': '^[0-9]{1,6}$','options': \"m\"}},\n",
        "    {'$regexMatch': {'input': \"$page_end\",'regex': '^[0-9]{1,6}$','options': \"m\"}}]}, 'references' : '$references','page_start' : '$page_start', 'page_end' : '$page_end' }}\n",
        "    projectReferencDetails= {'$project' : {'pages': {'$add': [{'$subtract': [{'$toInt': '$page_end'}, {'$toInt': '$page_start'}]}, 1]}, 'refcount' : { '$size' : '$references'} }}\n",
        "    bucketByRange = {'$bucket': {'groupBy': \"$pages\",'boundaries': [ 1, 6, 11, 16, 21, 26, 31 ],'default': \">30\",'output': {    'average':  { '$avg':'$refcount'} } } }\n",
        "    projectValues = {'$project' : {'_id':1, 'average' : { '$round' : ['$average',3]}}}\n",
        "    documents = articles.aggregate([projectPageDetails,checkReferences,projectReferencDetails,bucketByRange,projectValues])\n",
        "\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            histDict[str(doc['_id']) +'-'+ str(doc['_id']+4)]= doc['average']\n",
        "        except:\n",
        "            histDict[str(doc['_id'])] = doc['average']\n",
        "    return histDict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q6"
        ],
        "id": "VGo8-N1Fti5i"
      },
      "source": [
        "###  6)\n",
        "\n",
        "Being the competent data scientist you are, you remember that before sending the results to The Management you  should be verify that they are meaningful and not affected by data quality. Can you trust these averages are not affected by outliers?\n",
        "\n",
        "Task:\n",
        "\n",
        "Write a function to return for each article length range a list of outliers, that is,  articles with a z-score of number of references greater or equal than 3 ((https://www.statisticshowto.com/probability-and-statistics/z-score/)) \n",
        "\n",
        "Note that we say \"for each article length range\", therefore, z-score needs to be computed using the mean and stdev  of the provided article lengths. For Standard Deviation in Mongo, use https://docs.mongodb.com/v4.4/reference/operator/aggregation/stdDevPop/\n",
        "\n",
        "Example outpust is provided in a comment in the function\n",
        "\n",
        "[10 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:22:51.803520Z",
          "start_time": "2021-12-16T04:22:51.787959Z"
        },
        "tags": [
          "a6"
        ],
        "id": "bPW6-K8Lti5i"
      },
      "outputs": [],
      "source": [
        "def get_reference_outliers(articles):\n",
        "    \"\"\"\n",
        "    :param articles A PyMongo collection object\n",
        "    :return Dictionary of the form described below:\n",
        "     {\"1-5\" : {'outliers':[  \n",
        "             {          \n",
        "            id: article_id,           \n",
        "            num_references: number of references,           \n",
        "            z-score: z-score\n",
        "          }, ... and so on...], \n",
        "     \"6-10\" : {'outliers':[  \n",
        "             {          \n",
        "            id: article_id,           \n",
        "            num_references: number of references,           \n",
        "            z-score: z-score\n",
        "          }, ... and so on...] ,\n",
        "    .... and so on.....\n",
        "       }  \n",
        "    \"\"\" \n",
        "    checkReferences = {'$match': {'references': { '$exists' : True}, 'pageStatus' : True }}\n",
        "    projectPageDetails = {'$project' : { 'pageStatus' : { '$and' : [{'$regexMatch': {'input': \"$page_start\",'regex': '^[0-9]{1,6}$','options': \"m\"}},\n",
        "    {'$regexMatch': {'input': \"$page_end\",'regex': '^[0-9]{1,6}$','options': \"m\"}}]}, 'references' : '$references','page_start' : '$page_start', 'page_end' : '$page_end', 'id' : '$id' }}\n",
        "    projectReferencDetails= {'$project' : {'pages': {'$add': [{'$subtract': [{'$toLong': '$page_end'}, {'$toLong': '$page_start'}]}, 1]}, 'refcount' : { '$size' : '$references'}, 'id' : '$id' }}\n",
        "    bucketByRange = {'$bucket': {'groupBy': \"$pages\",'boundaries': [ 1, 6, 11, 16, 21, 26, 31 ],'default': \">30\",'output': { 'stddev': { '$stdDevPop':'$refcount'}, 'mean' : {'$avg': '$refcount'} ,'count' :{'$sum': 1} ,  'articles' : { '$push': { 'id' : '$id' , 'refcount' : '$refcount'}}} } }\n",
        "    unwindArticles= {'$unwind': {'path': '$articles'}}\n",
        "    projectZscore ={ '$project' : { '_id' : 1 , 'mean' : '$mean' , 'stdDev' : '$stddev','zscore' : { '$divide': [ {'$subtract': [{'$toInt': '$articles.refcount'}, {'$toInt': '$mean'}]}, {'$toInt': '$stddev'}]}, 'refcount' : '$articles.refcount', 'id' :'$articles.id'}}\n",
        "    groupByRange ={'$group' : {'_id' : '$_id' , 'outliers' :{ '$push' : { 'id' : '$id' , 'num_references' : '$refcount', 'z-score' : { '$round' : ['$zscore',3]} }}}}\n",
        "    matchZscore = {'$match': {'zscore' : {'$gte' : 3}}}\n",
        "    sortByRange = { '$sort' : {'_id' : 1}}\n",
        "    documents = articles.aggregate([projectPageDetails,checkReferences,projectReferencDetails,bucketByRange,unwindArticles,projectZscore,matchZscore,groupByRange,sortByRange], allowDiskUse = True)\n",
        "\n",
        "    zScoreDict = {}\n",
        "    for doc in documents:\n",
        "      outliersDict = {}\n",
        "      try:\n",
        "        outliersDict['outliers'] = doc['outliers']\n",
        "        zScoreDict[str(doc['_id']) +'-'+str(doc['_id']+4)] = outliersDict\n",
        "      except:\n",
        "        outliersDict['outliers'] = doc['outliers']  \n",
        "        zScoreDict[str(doc['_id'])] = outliersDict\n",
        "   \n",
        "    return zScoreDict\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q7"
        ],
        "id": "_11kJxrHti5i"
      },
      "source": [
        "### 7) \n",
        "\n",
        "What are the collaborators of an author? \n",
        "\n",
        "Task:\n",
        "\n",
        "Write a function that receives as input an author id and returns a list of authors that have at least one article co-authored with the input author. \n",
        "    \n",
        "Notes:\n",
        "* Assume authors are uniquely identified by their ids.\n",
        "* Affiliations are in the 'org' field of the author subdocuments, you need to rename that field in your output to conform to the expected format.\n",
        "* Sometimes affiliations come together in the same string value instead of different documents e.g. \"University of Southampton and University of Bournemouth\", consider this case as a single affiliation.\n",
        "* Remember a dictionary does not have an ordering in its keys\n",
        "* Some articles list authors only with name, without affiliation. Include this in your answer as a dictionary with a single key 'name'\n",
        "* Example output is provided in the definition of the function\n",
        "\n",
        "\n",
        "[8 points]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:25:25.663774Z",
          "start_time": "2021-12-16T04:25:25.646486Z"
        },
        "tags": [
          "a7"
        ],
        "id": "9nHL35ggti5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_collaborators(articles,author_id):\n",
        "    \"\"\"\n",
        "    Input: articles: PyMongo collection of articles\n",
        "        author_id : id of author\n",
        "    Output: List of collaborators. \n",
        "    Example output:\n",
        "    [\n",
        "    {'id': 2942936634,  'names': [\n",
        "           {'name': 'Lior Kovalio',\n",
        "           'affiliation': 'The Hebrew Univ. of Jerusalem, Jerusalem, Israel'},\n",
        "           {'name' : 'Lior Kovalio'}\n",
        "           ]\n",
        "    },\n",
        "    {'id': 2231056490, 'names': [\n",
        "           {'name': 'Gustavo F. Tondello', \n",
        "           'affiliation': 'University of Waterloo'},\n",
        "           { 'name': 'Gustavo F. Tondello', \n",
        "           'affiliation': 'Federal University of Santa Catarina'}\n",
        "           ]\n",
        "    },\n",
        "    {'id': 19031441 , 'names': [\n",
        "                  {'affiliation': 'Hebrew University of Jerusalem, Israel / '\n",
        "                            'Microsoft Research, Israel#TAB#',    \n",
        "                    'name': 'Noam Nisan'},\n",
        "                   {'affiliation': 'The Hebrew University of Jerusalem, Rachel & '\n",
        "                            'Selim Benin School of Computer Science & '\n",
        "                            'Engineering and Federmann Center for the Study of '\n",
        "                            'Rationality, Israel',\n",
        "                     'name': 'Noam Nisan'} \n",
        "                            ]\n",
        "     }\n",
        "     ]    \n",
        "    \"\"\"\n",
        "    #Your code here\n",
        "    findAuthor = {'$match': {'authors.id': author_id}}\n",
        "    projecrAtricleAuthorDtls = {'$project' : {'articleId':'$id', 'authors' : '$authors','coauthors' : '$authors.id'}}\n",
        "    unwindAuthors = {'$unwind' : {'path' : '$authors'}}\n",
        "    unwindCoauthors = {'$unwind' : {'path' : '$coauthors'}}\n",
        "    removeSelfAuthor = {'$match' : {'authors.id' : { '$ne' : author_id} }}\n",
        "    groupByAuthors = {'$group' : { '_id' : '$authors.id', 'names' : { '$addToSet' : { 'affiliation' : '$authors.org' ,'name' : '$authors.name'}}}}\n",
        "    documents = list(articles.aggregate([findAuthor,projecrAtricleAuthorDtls,unwindAuthors,unwindCoauthors,removeSelfAuthor,groupByAuthors]))   \n",
        "    \n",
        "    return documents\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q8"
        ],
        "id": "HrrsRHXiti5j"
      },
      "source": [
        "### 8)\n",
        "\n",
        "The Management anticipates that several queries based on authors and their connections will be required in the near future. You have been asked to take the necessary steps to ensure these queries are easier to express and faster to perform.\n",
        "\n",
        "Task : Create a collection named authors containing for each author a document with the following fields:\n",
        " * _id: author's id\n",
        " * name_affiliations: array of pairs (name ,affiliation) associated to _id\n",
        " * articles: array of artilcle ids authored by author with id = _id\n",
        " * collaborators: array of ids of collaborators, other authors that have written at least one article with _id\n",
        "\n",
        "We provide an authors.json file with a sample of the expected output. This file is in a slightly different format than articles.json, to load it, you need to include the --jsonArray option to your mongoimport command (see the documentation: https://docs.mongodb.com/database-tools/mongoimport/#cmdoption-mongoimport-jsonarray)\n",
        "\n",
        "[10 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:27:48.759065Z",
          "start_time": "2021-12-16T04:25:53.821481Z"
        },
        "tags": [
          "a8"
        ],
        "id": "Xojr1NKdti5j"
      },
      "outputs": [],
      "source": [
        "def create_authors_collection(articles):\n",
        "    \"\"\"\n",
        "    :param: articles collection\n",
        "    : output: Pymongo cursor of the newly created authors collection\n",
        "    {\n",
        "     '_id': author's id ,\n",
        "     'name_affiliations': array of  {name: name, affiliation: affiliation} associated to author's id ,\n",
        "      'articles': array of article_ids authored by this author id ,\n",
        "      'collaborators' : Array of ids of collaborators\n",
        "    }\n",
        "     Sample in authors.json\n",
        "    \"\"\"\n",
        "    projectRequiredDetails = {'$project' : {'_id' : 0, 'articles' : '$id', 'coauthors' : '$authors.id' , 'author' : '$authors'}}\n",
        "    unwindAuthors ={'$unwind': {'path' : '$author'}}\n",
        "    unwindCoauthors = {'$unwind' : {'path': '$coauthors'} }\n",
        "    projectCorrectNames = {'$project' : { 'articles':1,'coauthors' : 1,'authorid': '$author.id', 'name' : '$author.name','affiliation' : '$author.org'}}\n",
        "    removeSelfAuthors = { '$project' : {'articles' : 1, 'authorid' : 1, 'name': 1,'affiliation' : 1,'coauthors':{ '$cond':{'if':{'$eq':['$coauthors','$authorid']},'then':'$REMOVE','else':'$coauthors'} } }}\n",
        "    groupByAuthorId = {'$group' : {'_id' : '$authorid', 'articles' : {'$addToSet' : '$articles'}, 'name_affiliations' : {'$addToSet':{'name':'$name','affiliation':'$affiliation'}},'collaborators':{'$addToSet':'$coauthors'}}}\n",
        "    createCollection = {'$out' : 'authors'}\n",
        "    documents = articles.aggregate([projectRequiredDetails,unwindCoauthors,unwindAuthors,projectCorrectNames,removeSelfAuthors,groupByAuthorId,createCollection],allowDiskUse = True)\n",
        "\n",
        "    return documents\n",
        "    \n",
        "create_authors_collection(articles)\n",
        "authors = get_collection('coursework','authors')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "a",
          "q9"
        ],
        "id": "ixzR5o6ati5j"
      },
      "source": [
        "### 9)\n",
        "\n",
        "\n",
        "The Management is investigating the ties of authors with collaborators of different countries. They would like to know for a given author, to how many authors of the same country they could reach through colleagues of that country\n",
        "\n",
        "Task:\n",
        "\n",
        "Given an author id and the name of a country, return the ids of the authors that match the following conditions:\n",
        " 1) have at least one affiliation that includes the input country name\n",
        " 2) there is a path between this author and the input author where all nodes match condition (1)\n",
        " 3) at a distance of at most 3 from the input author id in the graph of co-authorship\n",
        "\n",
        "\n",
        "Notes:\n",
        "\n",
        "* Do not include authors without affiliation, or affiliations that do not explicitly include the name of the country. Consider those as not matching the query. \n",
        "\n",
        "* If you are stuck with question 8) you can use the authors.json sample provided for question 8 for development of questions 9). We will mark questions 9) by running them on the actual answer of Q8. Do note the actual answer of Q8 is much larger than the sample, take this in consideration when designing your solutions for Q9.\n",
        "\n",
        "* Watch out for authors with many articles (thus, a large network of collaborators)... They may consume all your memory...\n",
        "\n",
        "\n",
        "[10 points]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q9"
        ],
        "id": "zYRMADVbti5j"
      },
      "source": [
        "![Q8-example.png](attachment:Q8-example.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:29:16.513407Z",
          "start_time": "2021-12-16T04:29:16.506165Z"
        },
        "tags": [
          "a9"
        ],
        "id": "kn4GZo8gti5k"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_network(authors,author_id,country):\n",
        "    \"\"\"\n",
        "    Input: Authors collection, author_id, String country\n",
        "    Output: List of ids of authors at three or less hops of distance with an affiliation that includes 'country'\n",
        "    \"\"\"\n",
        "    #Your code here\n",
        "    matchAuthorId = {'$match' :  {'_id' :  author_id}}\n",
        "    graphLookup = {'$graphLookup' : {'from': \"authors\",  'startWith': \"$collaborators\", 'connectFromField' : 'collaborators' , 'connectToField' : '_id', 'as' : 'coauthorsDetails' ,   'maxDepth': 2 , 'depthField' : \"depth\", 'restrictSearchWithMatch' : {'name_affiliations.affiliation'  : {'$regex' :  country, '$options' : 'i'}}}}\n",
        "    unwindCoAUthors = {'$unwind' :  {'path' : '$coauthorsDetails'}}\n",
        "    projectAuthorIds = { '$project' : {'_id': 0, 'authorid' : '$coauthorsDetails._id' }}\n",
        "    documents = authors.aggregate([matchAuthorId,graphLookup,unwindCoAUthors,projectAuthorIds])\n",
        "    authorIdLst= []\n",
        "    for doc in documents:\n",
        "        authorIdLst.append(doc['authorid'])\n",
        "    return authorIdLst\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgmFAa5ti5k"
      },
      "source": [
        "### 10)\n",
        "\n",
        "The Management wants to know what articles are more important in the field of Machine Learning (look at the \"fos\" field for that). They know that papers in the references field may be from years previous to the ones in your dataset, so they would also like to know what is the most important paper within that subset published in each of the years you have in your dataset (2017,2018,2019). They want to use in-degree centrality as measure of importance.\n",
        "\n",
        "Task:\n",
        "\n",
        "Write a function that filters the subset of \"Machine learning\" articles and from it, compute the article with the highest in-degree centrality, both overall, and for articles published in each of the 2017, 2018 and 2019 years. \n",
        "\n",
        "Example output is provided as a comment in the function.\n",
        "\n",
        "\n",
        "[10 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:31:14.154541Z",
          "start_time": "2021-12-16T04:31:14.139779Z"
        },
        "tags": [
          "a10"
        ],
        "id": "gBohsEjnti5k"
      },
      "outputs": [],
      "source": [
        "\n",
        "def machine_learning_central(articles):\n",
        "    \"\"\"\n",
        "    Input: Articles collections.\n",
        "    Output : {\n",
        "       'overall' : (id_article_highest_indegree_centrality),\n",
        "       '2017' : id_article_highest_indegree_centrality published year 2017,\n",
        "       '2018' : id_article_highest_indegree_centrality published year 2018,\n",
        "       '2019' : id_article_highest_indegree_centrality published year 2019,\n",
        "    }\n",
        "    In the case that for a given year, no article has been referenced (and thus, all centrality values are zero), \n",
        "    put None as the value of that year, example:\n",
        "    {\n",
        "       'overall' : id_article_highest_indegree_centrality,\n",
        "       '2017' : id_article_highest_indegree_centrality of year 2017,\n",
        "       '2018' : id_article_highest_indegree_centrality of year 2018,\n",
        "       '2019' : None,\n",
        "    }\n",
        "    \"\"\"\n",
        "    #Your code here\n",
        "    machineLearningDict = {}\n",
        "    checkFos = {'$match': {\"$and\": [{'fos': { '$exists' : True} }, {'references': { '$ne': None}}]}}\n",
        "    unwindFos = {'$unwind': {'path' : '$fos'}}\n",
        "    filterMachinieLearningArticles = {'$match' : {'fos.name' : {'$regex' : 'machine learning' , '$options' : 'i'}}}\n",
        "    groupByYear = {'$group' : { '_id' : '$year','articleDetails' : {'$addToSet' : { 'articleId' : '$id', 'references' : '$references'}}}}\n",
        "    unwindArticleDetails = {'$unwind' :{'path' : '$articleDetails'}}\n",
        "    unwindReferencesDetails = {'$unwind' :{'path' : '$articleDetails.references'}}\n",
        "    projectDetails = {'$project' : {'_id':0, 'year':'$_id', 'articleId' : '$articleDetails.articleId', 'references' : '$articleDetails.references'}}\n",
        "    documents = list(articles.aggregate([checkFos,unwindFos,filterMachinieLearningArticles,groupByYear,unwindArticleDetails,unwindReferencesDetails,projectDetails]))\n",
        "    articlesDataFrame = pd.DataFrame(documents)\n",
        "    machineLearningDict['overall'] = calculateInDegreeCentrality(articlesDataFrame,'references','articleId')\n",
        "    machineLearningDict['2017'] = calculateInDegreeCentrality(articlesDataFrame[articlesDataFrame['year'] == 2017],'references','articleId')\n",
        "    machineLearningDict['2018'] = calculateInDegreeCentrality(articlesDataFrame[articlesDataFrame['year'] == 2018],'references','articleId')\n",
        "    machineLearningDict['2019'] = calculateInDegreeCentrality(articlesDataFrame[articlesDataFrame['year'] == 2019],'references','articleId')\n",
        "    return machineLearningDict\n",
        "\n",
        "         \n",
        "def calculateInDegreeCentrality(articlesDataFrame,references,articleId):\n",
        "   diGraph = nx.from_pandas_edgelist(articlesDataFrame,source=articleId,target=references, edge_attr=True, create_using=nx.DiGraph())\n",
        "   centralityDegree = nx.in_degree_centrality(diGraph)\n",
        "   max_value = max(centralityDegree.values())\n",
        "   max_key = [i for i in centralityDegree.keys() if centralityDegree[i]==max_value]\n",
        "   return max_key[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQsoNZA1ti5k"
      },
      "source": [
        "## Task 2\n",
        "This task will assess your ability to use the Hadoop Streaming API and MapReduce to process data. For each of the questions below, you are expected to write two python scripts, one for the Map phase and one for the Reduce phase. You are also expected to provide the correct parameters to the `hadoop` command to run the MapReduce process. Write down your answers in the specified cells below.\n",
        "\n",
        "You will use the same dataset of articles that you used for task 1.\n",
        "\n",
        "To help you, `%%writefile` has been added to the top of the cells, automatically writing them to \"mapper.py\" and \"reducer.py\" respectively when the cells are run.\n",
        "\n",
        "No need to return a Python dictionary, the expected output here is the output file of Hadoop, named \"output\" for question1 and \"output2\" for question (do not change that part of the question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q11"
        ],
        "id": "j6FGErpZti5k"
      },
      "source": [
        "### 1) \n",
        "\n",
        "Answer Question 2) of task 1 using the MapReduce paradigm (repeated below for your convenience)\n",
        "\n",
        "Return the set of publishers of articles that are missing the doc_type property or have it equal to the empty string. \n",
        "\n",
        "[8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:32:50.161394Z",
          "start_time": "2021-12-16T04:32:50.146200Z"
        },
        "tags": [
          "mapper11"
        ],
        "id": "wKTNWngsti5k"
      },
      "outputs": [],
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "# Answer for mapper.py\n",
        "import json\n",
        "import sys\n",
        "\n",
        "for eachJsonDocument in sys.stdin:\n",
        "    publisher=''\n",
        "    eachArticle = json.loads(eachJsonDocument)\n",
        "    if (('doc_type' not in eachArticle or eachArticle['doc_type'] == '') and ( 'publisher' in eachArticle)):\n",
        "        try:\n",
        "            publisher = eachArticle['publisher']\n",
        "        except:\n",
        "            continue\n",
        "        print(publisher+'\\t1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:32:51.839649Z",
          "start_time": "2021-12-16T04:32:51.831687Z"
        },
        "scrolled": true,
        "tags": [
          "reducer11"
        ],
        "id": "-1ATU9Jfti5k"
      },
      "outputs": [],
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "# Answer for reducer.py\n",
        "import json\n",
        "import sys\n",
        "setOfPublishers = set()\n",
        "inputPairs = sys.stdin.readlines()\n",
        "\n",
        "for row in inputPairs:\n",
        "    keyValuePair = row.split('\\t',1)\n",
        "\n",
        "    if ( keyValuePair[0] not in setOfPublishers):\n",
        "        setOfPublishers.add(keyValuePair[0])\n",
        "        print (keyValuePair[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:38:00.990347Z",
          "start_time": "2021-12-16T04:37:19.551394Z"
        },
        "id": "Kbd4GPsmti5k"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Hadoop command to run the map reduce.\n",
        "rm -r output\n",
        "hadoop-standalone-mode.sh\n",
        "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
        "-files mapper.py,reducer.py \\\n",
        "-input ~/datasets/articles.json \\\n",
        "-mapper ./mapper.py \\\n",
        "-reducer ./reducer.py \\\n",
        "-output output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0g_rNCwti5k"
      },
      "outputs": [],
      "source": [
        "#Expected output format, list of publishers, one per line\n",
        "#Elsevier India\n",
        "#Springer Cham\n",
        "#UK Academy of Sciences\n",
        "#....\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "q12"
        ],
        "id": "xlRlBwlyti5l"
      },
      "source": [
        "### 2) \n",
        "Answer Question 5) of task 1 using the MapReduce paradigm (repeated below for your convenience)\n",
        "\n",
        "Do longer articles (in number of pages) have more references?\n",
        "\n",
        "Task:\n",
        "\n",
        "Return an histogram dictionary with the following specification:\n",
        "\n",
        "{\n",
        "\"1-5\" : Average references of articles between 1 and 5 pages inclusive\n",
        "\n",
        "\"6-10\" : Average references of articles between 6 and 10 pages inclusive\n",
        "\n",
        "\"11-15\" : Average references of articles between 11 and 15 pages inclusive\n",
        "\n",
        "\"16-20\" : Average references of articles between 16 and 20 pages inclusive\n",
        "\n",
        "\"21-25\" : Average references of articles between 21 and 25 pages inclusive\n",
        "\n",
        "\"26-30\" : Average references of articles between 26 and 30 pages inclusive\n",
        "\n",
        "\">30\" : Average references of articles with more than 30 pages\n",
        "}\n",
        "\n",
        "A fellow data scientist of your team has found that some articles have no 'references' field, while others have unusually large page numbers. The Management decides that for this task you should\n",
        "\n",
        "* Ignore articles that are missing the 'references' field\n",
        "* Ignore articles with page numbers greater than 6 digits\n",
        "* Ignore articles with page numbers that cannot be parsed to integers (e.g. 900467-12)\n",
        "\n",
        "Clarifications:\n",
        "*) Articles that start and end at the same page have 1 page, not zero!\n",
        "\n",
        "\n",
        "[12 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:38:16.630155Z",
          "start_time": "2021-12-16T04:38:16.622355Z"
        },
        "tags": [
          "mapper12"
        ],
        "id": "WvyNgkglti5l"
      },
      "outputs": [],
      "source": [
        "%%writefile mapper2.py\n",
        "#!/usr/bin/env python\n",
        "# Answer for mapper.py\n",
        "import json\n",
        "import sys\n",
        "\n",
        "for eachJsonDocument in sys.stdin:\n",
        "    eachArticle = json.loads(eachJsonDocument)\n",
        "    if ( ('references' in eachArticle) and \n",
        "        ( ('page_start' in eachArticle) and (eachArticle['page_start'].isnumeric()) and( len(eachArticle['page_start']) <=6 ) ) and \n",
        "         ( ('page_end' in eachArticle) and (eachArticle['page_end'].isnumeric()) and( len(eachArticle['page_end']) <=6 ) )\n",
        "        ) :\n",
        "        noOfPage = (int(eachArticle['page_end']) - int (eachArticle['page_start'])) + 1\n",
        "        if( noOfPage >= 1 and noOfPage <=5):\n",
        "            print ( '1-5'+'\\t'+str(len(eachArticle['references'])))\n",
        "        elif (noOfPage >= 6 and noOfPage <=10):\n",
        "            print ( '6-10'+'\\t'+str(len(eachArticle['references'])))\n",
        "        elif (noOfPage >= 1 and noOfPage <=15):\n",
        "            print ( '11-15'+'\\t'+str(len(eachArticle['references'])))\n",
        "        elif (noOfPage >= 16 and noOfPage <=20):\n",
        "            print ( '16-20'+'\\t'+str(len(eachArticle['references'])))\n",
        "        elif (noOfPage >= 21 and noOfPage <=25):\n",
        "            print ( '21-25'+'\\t'+str(len(eachArticle['references'])))\n",
        "        elif (noOfPage >= 26 and noOfPage <=30):\n",
        "            print ( '26-30'+'\\t'+str(len(eachArticle['references'])))\n",
        "        elif (noOfPage >30):\n",
        "            print ( '>30'+'\\t'+str(len(eachArticle['references'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:38:32.592485Z",
          "start_time": "2021-12-16T04:38:32.586287Z"
        },
        "tags": [
          "reducer12"
        ],
        "id": "IsZfMTQBti5l"
      },
      "outputs": [],
      "source": [
        "%%writefile reducer2.py\n",
        "#!/usr/bin/env python\n",
        "# Answer for reducer.py\n",
        "import json\n",
        "import sys\n",
        "\n",
        "from collections import defaultdict\n",
        "accumulatorRefLength = defaultdict(lambda: 0)\n",
        "accumulatorCount = defaultdict(lambda: 0)\n",
        "rangeDict = {}\n",
        "\n",
        "inputPairs = sys.stdin.readlines()\n",
        "\n",
        "for row in inputPairs:\n",
        "    keyValuePair = row.split('\\t',1)\n",
        "    accumulatorRefLength[keyValuePair[0]] =  accumulatorRefLength[keyValuePair[0]] + int(keyValuePair[1].strip())\n",
        "    accumulatorCount[keyValuePair[0]] = accumulatorCount[keyValuePair[0]]+1\n",
        "\n",
        "for key,value in accumulatorRefLength.items():\n",
        "    accumulatorRefLength[key] = round(value/accumulatorCount[key],3)\n",
        "accumulatorRefLength = {k: v for k, v in sorted(accumulatorRefLength.items(), key=lambda item: item[1])}\n",
        "\n",
        "for key,value in accumulatorRefLength.items():\n",
        "    print(key+'\\t'+str(value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-16T04:57:17.518331Z",
          "start_time": "2021-12-16T04:56:10.352877Z"
        },
        "id": "JU0TtBUJti5l"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Hadoop command to run the map reduce.\n",
        "rm -r output2\n",
        "hadoop-standalone-mode.sh\n",
        "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
        "-files mapper2.py,reducer2.py \\\n",
        "-input ~/datasets/articles.json \\\n",
        "-mapper ./mapper2.py \\\n",
        "-reducer ./reducer2.py \\\n",
        "-output output2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1skloOOOti5l"
      },
      "outputs": [],
      "source": [
        "#Expected key-value output format:\n",
        "#1-5    average\n",
        "#6-10   average\n",
        "#... and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGkEL9lHti5l"
      },
      "source": [
        "### Marking scheme\n",
        "\n",
        "- The result provided is correct: An incorrect answer will have between 0% and 40% of the mark depending on the nature of the mistake. Questions where there was only one answer possible will have 0%, questions where the result is correct in some cases and not others will be marked at 20% or 40%. Feel free to create as many notebooks as you want for experimenting and transcribe your final answer to the one you submit.\n",
        "\n",
        "- The result is provided in the expected format and output: 20% will be deducted to correct results that are not in the expected format\n",
        "\n",
        "- Efficiency of the answer: Measured in terms of execution time. There are many ways to reach the correct result, some of them are more efficient than others, some are more straight forward than others. \n",
        "\n",
        "- Tables below detail the percentage of mark you get according to the efficiency of the answer, each cell shows the maximum time allowed to get the mark in the corresponding row. Answers that take more time than the time in the 60% column will be declared timeout and get zero points. (run the cells for showing the tables)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8k8Fm9Gti5l"
      },
      "source": [
        "\n",
        "\n",
        "| Task1 | Points |  100% | 80% | 60% | \n",
        "| --- | --- | --- | --- | --- | \n",
        "| q1 | 5 | 5sec |  10 sec | 2minutes | \n",
        "|  q2 | 5 | 5sec   | 10 sec   | 2minutes   |\n",
        "|  q3 | 5 |  10sec  | 40sec  |  5minutes  |\n",
        "|  q4 | 5 | 5sec   | 10sec   | 2minutes   |\n",
        "|  q5 | 10 | 10sec   | 60sec  | 8minutes   |\n",
        "|  q6 | 10  |  15sec   | 90sec   | 10minutes   |\n",
        "|  q7 | 10 | 15sec    | 90sec   | 10minutes   |\n",
        "|  q8 | 10 | 90 sec    | 5minutes  | 10minutes  | \n",
        "|  q9 (worst case) | 10   | 20sec   |  100 sec  | 10 minutes  |\n",
        "|  q10| 10  |  60sec  | 3minutes   | 10minutes   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-10-27T03:16:46.397Z"
        },
        "id": "OAZGCJHOti5l"
      },
      "source": [
        "| Task2 | points | 100% | 80% | 60% | \n",
        "| --- |--- | --- | --- | --- | \n",
        "| q1 | 8 | 90sec |  5minutes | 10minutes | \n",
        "|  q2| 12  | 90sec   | 5minutes   | 10minutes   |"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4Nr8-oztti5b"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}